{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535cfc19-adb4-4502-8a5d-7f7482b12ea5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Validador dos Arquivos Carregados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ac0183-ab15-4ae3-8f79-a04da4f6edb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Definições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e364a3a6-0d88-444a-9651-93c0590af995",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importando bibliotecas necessárias\n",
    "from pyspark.sql.functions import regexp_replace, to_date, split, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6f606e-80be-46d6-9833-bd79ace1f63b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# general - csse_covid_19_daily_reports\n",
    "url_github = 'https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports'\n",
    "\n",
    "# us - csse_covid_19_daily_reports_us\n",
    "url_github_us = 'https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports_us'\n",
    "\n",
    "# lake\n",
    "caminho_lake_general = 'dbfs:/mnt/projetointegrador/raw/general'\n",
    "caminho_lake_us = 'dbfs:/mnt/projetointegrador/raw/us'\n",
    "\n",
    "\n",
    "# partição \n",
    "particao = 'DATE_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43c5f7b3-9f73-4f36-81b6-8e25c15d3395",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_diretorio(caminho):\n",
    "    try:\n",
    "        dbutils.fs.ls(caminho)\n",
    "        print('Pasta já existe: ' + caminho)\n",
    "    except: \n",
    "        dbutils.fs.mkdirs(caminho)\n",
    "        print('Pasta criada: ' + caminho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e46ed55-ed49-42cf-9dba-eff609f3f448",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# função que conta quantos arquivos tem carregado no Github\n",
    "def contagem_arquivos_git(url_github, tipo):\n",
    "\n",
    "    result = requests.get(url_github)\n",
    "    soup = BeautifulSoup(result.text, 'html.parser')\n",
    "    csvfiles1 = soup.find_all(title=re.compile(\"\\.csv$\"))\n",
    "\n",
    "    df1 = pd.DataFrame(csvfiles1, columns=['Cam_Git'])\n",
    "    df1['DATE_KEY'] = df1['Cam_Git'].str.replace('.csv', '', case=False)\n",
    "    df1['Tipo'] = tipo\n",
    "\n",
    "    df = spark.createDataFrame(df1) \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d41801-50a7-423c-8726-3b68a9375df1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# função que conta quantos arquivos foram carregados no lake\n",
    "def contagem_arquivos_lake(caminho_lake, particao):\n",
    "    #caminho_lake = 'dbfs://' + caminho_lake\n",
    "    # obtendo lista de partições da fato\n",
    "    lista_part_fato = dbutils.fs.ls(caminho_lake)\n",
    "    #print(lista_part_fato)\n",
    "\n",
    "    schema_vazio = StructType([StructField(\"coluna_vazia\", StringType(), nullable=True)])\n",
    "\n",
    "    # checando se a lista está preenchida\n",
    "    if not lista_part_fato:        \n",
    "        return spark.createDataFrame([], schema_vazio)\n",
    "    else:        \n",
    "        # caso esteja preenchida, obter os nomes das partições\n",
    "        df_part_fato = spark.createDataFrame(data = lista_part_fato).select('name')\n",
    "\n",
    "        \n",
    "        # separando o mês o mês do texto\n",
    "        df_part_fato = (\n",
    "            df_part_fato\n",
    "                #.withColumn('calendar_key_filter', split(df_part_fato['name'], '=').getItem(0))\n",
    "                .withColumn(\"DATE_KEY\", regexp_replace(df_part_fato['name'], '.csv', ''))\n",
    "                .withColumnRenamed('name', 'Cam_Lake')\n",
    "                #.withColumnRenamed('name', 'DATE_KEY')\n",
    "        )        \n",
    "        # filtrando apenas partições\n",
    "        #df_part_fato = df_part_fato.filter(df_part_fato.calendar_key_filter == 'DATE_KEY')\n",
    "        \n",
    "\n",
    "        # checando de df tem valor\n",
    "        if df_part_fato.count() == 0:\n",
    "            return spark.createDataFrame([], schema_vazio)\n",
    "        else:\n",
    "            return df_part_fato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57334e56-e81e-4852-986c-6f28ad291941",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Validador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904ade67-3bc3-4c67-ad71-b9c6d7dd84bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Checando se pasta no lake existe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8f29e4-45f2-42bb-853f-ff2c44b865ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta já existe: dbfs:/mnt/projetointegrador/raw/general\nPasta já existe: dbfs:/mnt/projetointegrador/raw/us\n"
     ]
    }
   ],
   "source": [
    "# rodando função para checar a existencia da estrutura, caso não exista, crie.\n",
    "check_diretorio(caminho_lake_general)\n",
    "\n",
    "# rodando função para checar a existencia da estrutura, caso não exista, crie.\n",
    "check_diretorio(caminho_lake_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03104e93-96b0-48c4-a6f6-196a8ff23d78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Carregando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17364a13-dca8-4c57-8d0a-ff4e802b0731",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<command-1123838663228577>:9: FutureWarning: The default value of regex will change from True to False in a future version.\n  df1['DATE_KEY'] = df1['Cam_Git'].str.replace('.csv', '', case=False)\n<command-1123838663228577>:9: FutureWarning: The default value of regex will change from True to False in a future version.\n  df1['DATE_KEY'] = df1['Cam_Git'].str.replace('.csv', '', case=False)\n"
     ]
    }
   ],
   "source": [
    "# arquivos lake\n",
    "\n",
    "# general\n",
    "df_arquivos_lake_general = contagem_arquivos_lake(caminho_lake_general, particao)\n",
    "df_arquivos_git_general = contagem_arquivos_git(url_github, 'General') # arquivos github\n",
    "\n",
    "# us\n",
    "df_arquivos_lake_us = contagem_arquivos_lake(caminho_lake_us, particao)\n",
    "df_arquivos_git_us = contagem_arquivos_git(url_github_us, 'US') # arquivos github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de1e356-33c7-48f6-bdce-026b423384a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Obtendo datas faltantes, caso haja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e85959a-b9ed-42f6-9ace-e7145e0e9d6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# General\n",
    "check_general = df_arquivos_lake_general.count() == 0 \n",
    "check_us = df_arquivos_lake_us.count() == 0 \n",
    "\n",
    "if check_general:\n",
    "    faltantes_general = (\n",
    "        df_arquivos_git_general\n",
    "            .select(*['DATE_KEY'])\n",
    "    )\n",
    "else:\n",
    "    faltantes_general = df_arquivos_git_general.join(df_arquivos_lake_general, 'DATE_KEY', 'left')\n",
    "    faltantes_general = (\n",
    "        faltantes_general\n",
    "            .where('Cam_Lake is NULL')\n",
    "            .select(*['DATE_KEY'])\n",
    "    )\n",
    "\n",
    "# US\n",
    "\n",
    "if check_us:\n",
    "    faltantes_us = (\n",
    "        df_arquivos_git_us\n",
    "            .select(*['DATE_KEY'])\n",
    "    )\n",
    "else:\n",
    "    faltantes_us = df_arquivos_git_us.join(df_arquivos_lake_us, 'DATE_KEY', 'left')\n",
    "    faltantes_us = (\n",
    "        faltantes_us\n",
    "            .where('Cam_Lake is NULL')\n",
    "            .select(*['DATE_KEY'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4d00f9-c3f6-4ffe-9982-4c02c9664121",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Gerando datas que faltam carregar no Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f38620ee-f9d7-4ee9-8790-a1e2f33d5ab8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# General\n",
    "(\n",
    "    faltantes_general\n",
    "       .coalesce(1)\n",
    "        .write\n",
    "        .format(\"csv\")\n",
    "        .mode(\"overwrite\")\n",
    "        .options(delimiter='|')\n",
    "        .option(\"header\", True)\n",
    "        .save(\"/mnt/projetointegrador/raw/datas_faltantes/datas_general\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcac94e5-3f90-4653-9807-080b0e36d4de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# US\n",
    "(\n",
    "    faltantes_us\n",
    "       .coalesce(1)\n",
    "        .write\n",
    "        .format(\"csv\")\n",
    "        .mode(\"overwrite\")\n",
    "        .options(delimiter='|')\n",
    "        .option(\"header\", True)\n",
    "        .save(\"/mnt/projetointegrador/raw/datas_faltantes/datas_us\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_validador_arquivos_carregados",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
